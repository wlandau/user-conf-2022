---
title: "data version control for the {targets} R package"
author: "Will Landau"
output:
  xaringan::moon_reader:
    nature:
      highlightLines: yes
---

<style>
.inverse {
background-color: transparent;
text-shadow: 0 0 0px transparent;
}
.title-slide {
vertical-align: bottom !important; 
text-align: center !important;
}
.title-slide h1 {
position: absolute;
top: -50px;
left: 0;
right: 0;
width: 100%;
line-height: 4em;
color: #666666;
}
.title-slide h2 {
position: absolute;
top: 0px;
right: 0;
left: 0;
line-height: 6em;
color: #666666;
}
.title-slide h3 {
position: absolute;
top: 400px;
right: 0;
left: 0;
line-height: 6em;
color: #666666;
}
.title-slide {
background-color: white;
background-image: url('images/gittargets.png');
background-repeat: no-repeat;
background-size: 25%;
background-position: 25px 0px 0px 0px;
}
.remark-slide-content:after {
content: "Copyright Eli Lilly and Company";
position: absolute;
bottom: -5px;
left: 20px;
height: 40px;
width: 100%;
font-family: Helvetica, Arial, sans-serif;
font-size: 0.7em;
color: #666666;
background-repeat: no-repeat;
background-size: contain;
}
</style>

```{r, include = FALSE, echo = FALSE}
set.seed(0)
suppressPackageStartupMessages({
  library(gittargets)
  library(targets)
})
knitr::opts_chunk$set(
  cache = FALSE,
  comment = "#>",
  fig.width = 10, 
  fig.height = 5
)
```

# Demanding computation in R

* **Bayesian data analysis: JAGS, Stan, NIMBLE, `greta`**
* Deep learning: `keras`, `tensorflow`, `torch`
* Machine learning: `tidymodels`
* PK/PD: `nlmixr`, `mrgsolve`
* Clinical trial simulation: `rpact`, `Mediana`
* Statistical genomics
* Social network analysis
* Permutation tests
* Database queries: `DBI`
* Big data ETL

???

A lot of work that happens in R is computationally demanding. Everything from machine learning to Bayesian statistics to ETL.

---

# Typical notebook-based project

![](./images/notebook-start.png)

???

Most people organize a project using numbered scripts or notebooks. These notebooks run in a sequential order. Each notebook writes output files, and these output files become the input files of the next notebook.

---

# The complicated reality

![](./images/notebook-end.png)

???

This approach does not scale, and the end product is not reproducible. You have to break the linear sequence in order to add steps. Intermediate files are cumbersome to micromanage, the code files get messy, the whole project gets messy, and it's too easy to lose track of what you're doing.

---

# Pipeline tools

![](./images/pipeline_graph.png)

* Orchestrate interdependent moving parts.
* Scale the computation.
* Manage results files.

???

Pipeline tools solve these problems. A pipeline tool orchestrates the overall flow of tasks from beginning to end, running the correct steps in the correct order and conditionally independent tasks in parallel. And most pipeline tools manage output. So for each task, you can trust that all the requisite data files are available and up to date.

---

# Types of pipeline tools

* Existing pipeline tools: https://github.com/pditommaso/awesome-pipeline
* Most are language-agnostic or designed for Python or the shell.
* Different pipline toolkits have different priorities and tradeoffs:

![](./images/tooltypes.png)

???

Beyond that, different pipeline tools have different priorities. Apache Airflow and GNU Make are two examples. Airflow cares about data pipelines that run on a regular schedule, need to run completely from end to end every time, and need a complete historical record of all past results. Make, on the other hand, only cares about the *current* state of the input and output data, and it cares that the output is synchronized with the code and input it came from. If an output artifact is up to date with its dependencies, then Make will not bother to run that task. This really comes in handy for a data analysis project under active development. If your pipeline tool automatically figures out what to rerun and what to skip, you save an *enormous* amount of time, and you have the peace of mind that your input and output and code are synchronized, up to date, and reproducible.

---

# {targets}

<center>
<img src="./images/targets.png" height = "400px">
</center>

* Make-like pipeline tool for R.
* New developments add Airflow-like capabilities.

???

The {targets} R package is a Make-like tool. It has the benefits of Make, such as skipping tasks that are already up to date. And it also preserves the look and feel of an R session. It nudges users toward good programming practices for the R language, and the interface abstracts away output data files so they look and feel like ordinary variables in R.

---

# Ecosystem around {targets}

<center>
<img src="./images/targetopia.png" height = "400px">
</center>

???

{targets} is extensible. Third-party interfaces make it easy to use {targets} for literate programming, heavy branching, and Bayesian data analysis.

---

# {targets} data store

```
_targets/ # Can be customized with tar_config_set().
├── meta/
├────── meta
├────── process
├────── progress
├── objects/
├────── target1 
├────── target2
├────── branching_target_c7bcb4bd
├────── branching_target_285fb6a9
├────── branching_target_874ca381
├── scratch/ # tar_make() deletes this folder after it finishes.
└── user/ # custom user storage for gittargets data version control.
```

???

The data storage system is simple. There is one file per target in the `objects` sub-folder, and the `tar_read()` function brings the data back into the R session. Multiple storage formats and modes of compression are available, including the Apache arrow formats like feather and parquet.

---

# Default data store tradeoffs are Make-like

<center>
<img src="./images/scale.png" height = "300px">
</center>

* Compactly represents the *current* state of the pipeline.
* Does not contain historical data from past runs.

???

This storage system is simple, lightweight, and completely aligned with the goals of {targets}. But on its own, it does have limitations. Like a typical Make-like tool, new output overwrites old output, and you only have the *current* state of the pipeline. Unless you take proactive steps to archive historical data files, they disappear.

---

# Current pipeline is up to date...

```{r, eval = FALSE}
# _targets.R
library(targets)
list(
  tar_target(data, airquality),
  tar_target(model, lm(Ozone ~ Wind, data = data))
)
```

```{r, eval = FALSE}
tar_make() # All targets are up to date.
#> ✓ skip target data
#> ✓ skip target model
#> ✓ skip pipeline
```

???

This gets tricky when you put the code under a version control system like Git. Here, we have an up-to-date targets pipeline. 

---

# But checking out old code invalidates a target!

```{r, eval = FALSE}
gert::git_branch_checkout(branch = "old-code-branch")
```

```{r, eval = FALSE}
# _targets.R
library(targets)
list(
  tar_target(data, airquality),
  tar_target(model, lm(Ozone ~ Temp, data = data)) # old model #<<
)
```

```{r, eval = FALSE}
tar_make() # The model is not up to date with the checked-out code.
#> ✓ skip target data
#> • start target model # (Didn't I already run this ages ago?) #<<
#> • built target model
#> • end pipeline
```

???

If you check out an old version of the *code*, and the data is still synced with a later version of the code, those targets are no longer up to date.

Now, how do we keep the synchronization and time savings of a Make-like tool, but also get the history and versioning of an Airflow-like tool? The ecosystem around {targets} supports two major options.

---

# Solution 1: cloud storage

```{r, eval = FALSE}
# _targets.R file
library(targets)
tar_option_set(resources = tar_resources(
  aws = tar_resources_aws(bucket = "my_versioned_bucket")
))
list(
  tar_target(data, get_data(), repository = "aws"), # or "gcp"
  tar_target(model, run_model(data), repository = "aws") # or "gcp"
)
```

* How to configure a cloud-backed version-aware pipeline:
    1. Create a bucket (either Amazon or Google) and turn on versioning.
    2. Supply the bucket to target resources.
    3. Select `repository = "aws"` (Amazon) or `"gcp"` (Google).
    4. Commit the metadata file `_targets/meta/meta` to source code version control (Git).
* Check out an old commmit to reproducibly roll back the pipeline. Recovers code and metadata together so targets stay up to date.
* Details: <https://books.ropensci.org/targets/data.html#cloud-storage>.
* Thanks [Mark Edmondson](https://github.com/MarkEdmondson1234) for GCP support.

???

First, the {targets} package itself supports storage in the cloud. If you supply a bucket in Amazon Web Services or Google Cloud, you can send target data there instead of your local machine. And if you turn on versioning in your bucket, {targets} automatically stores the version ID of each results file. That means if you track the project's metadata file in the same version control repository as the source code, all your past runs will be synchronized and available. In other words, when you check out an old Git commit, your data will roll back too, and your targets will stay up to date. And because up-to-date targets are skipped, this approach avoids duplicating output data like an Airflow tool might if it runs the full pipeline from scratch each time.

---

# Solution 2: {gittargets}

<center>
<img src="./images/gittargets.png" height = "300px">
</center>

* Put the data store under version control.
* Sync code commits with data commits.
* Switch commits and branches without invalidating the pipeline.
* Keep a historical record of the data from key milestones of the project.
* Documentation: https://docs.ropensci.org/gittargets/

???

For those of you without access to Amazon Web Services or Google Cloud, the {gittargets} R package is a completely local solution. {gittargets} creates a Git repository for the data store that stays synchronized with the Git repository of your source code. That way, when you check out an old commit in the code, it is straightforward to pull up the data snapshot that matches the commit. 

---

# {gittargets} workflow

1. `targets::tar_make()` (or `tar_make_clustermq()` or `tar_make_future()`): run pipeline at least once.
2. `gert::git_commit()`: commit code.
3. `gittargets::tar_git_init()`: initialize data repository.
4. `gittargets::tar_git_snapshot()`: create a data snapshot for the current code commit.
5. `gittargets::tar_git_checkout()`: revert the data to the appropriate prior snapshot.

???

Here's how it works. First, run the pipeline and commit your code to version control. Your targets should be up to date in the pipeline, and your code should be up to date in Git. Next, use tar_git_init() to create a Git repository for the {targets} data store, and use tar_git_snapshot() to commit the data files. Now, if you ever need to roll back the code to this commit, you can use tar_git_checkout() to roll back the data. If all goes well, you will have an older but still up-to-date set of targets.

Let's demonstrate. 

---

# Initialize data repository

* Assumes the pipeline ran at least once.

```{r, eval = FALSE}
tar_git_init()
#> ✔ Created data store Git repository
#> ✔ Wrote to _targets/.gitattributes for git-lfs: <https://git-lfs.github.com>.
#> ✔ Created stub commit without data.
#> • Run tar_git_snapshot() to put the data files under version control.
```


???

Initializing the data repository is straightforward,

---

# Snapshot the data


```{r, eval = FALSE}
tar_git_snapshot()
#> • Creating data branch code=af36d7301844831adc4cdedffbcb802cecebb0d1.
#> • Staging data files.
#> ✔ Staged 6 files in the data store.
#> • Committing data changes.
#> ✔ Created new data snapshot b0641526e062fee82e5d3d2c88163ea4823bc1b8.
#> • Packing references.
```

???

and so is snapshotting the data.

---

# Snapshot model

<center>
<img src="./images/snapshot-model-git.png" height = "450px">
</center>

???

A snapshot is a special git commit in the data that links to a commit in the code. All the data snapshots that match a code commit are in a branch with the code commit hash.

---

# Check out old code

```{r, eval = FALSE}
gert::git_branch_checkout("old-code-branch")
```

* Find code commits with data snapshots:

```{r, eval = FALSE}
tar_git_log()
#> # A tibble: 2 × 6
#>   message_code  message_data time_code           time_data           commit_code
#>   <chr>         <chr>        <dttm>              <dttm>              <chr>      
#> 1 Begin analyz… Begin analy… 2022-02-13 01:32:53 2022-02-13 01:32:55 af36d73018…
#> 2 Switch to UK… Switch to U… 2022-02-13 01:32:56 2022-02-13 01:32:57 e650867f85…
#> # … with 1 more variable: commit_data <chr>
```

???

When you check out an old branch, you may or may not have a data snapshot for the code commit you switched to. If you are unsure, you can call tar_git_log() to find out.

---

# Check out matching old data

```{r, eval = FALSE}
tar_git_checkout()
#> ✔ Checked out data snapshot b0641526e062fee82e5d3d2c88163ea4823bc1b8.
#> • Code commit: code=af36d7301844831adc4cdedffbcb802cecebb0d1
#> • Message: Begin analyzing the airquality dataset
#> • Resetting to HEAD of checked-out snapshot.
```

```{r, eval = FALSE}
tar_make() # The old data files are synced with the old code.
#> ✓ skip target data
#> ✓ skip target model
#> ✓ skip pipeline
```

???

If you do have a matching snapshot, you can call tar_git_checkout() to pull up the data that matches the code commit. And now, you have an prior set of targets that matches this historical state of the code. 
