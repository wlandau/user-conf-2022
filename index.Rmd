---
title: "data version control for the {targets} R package"
author: "Will Landau"
output:
  xaringan::moon_reader:
    nature:
      highlightLines: yes
---

<style>
.inverse {
background-color: transparent;
text-shadow: 0 0 0px transparent;
}
.title-slide {
vertical-align: bottom !important; 
text-align: center !important;
}
.title-slide h1 {
position: absolute;
top: -50px;
left: 0;
right: 0;
width: 100%;
line-height: 4em;
color: #666666;
}
.title-slide h2 {
position: absolute;
top: 0px;
right: 0;
left: 0;
line-height: 6em;
color: #666666;
}
.title-slide h3 {
position: absolute;
top: 400px;
right: 0;
left: 0;
line-height: 6em;
color: #666666;
}
.title-slide {
background-color: white;
background-image: url('images/gittargets.png');
background-repeat: no-repeat;
background-size: 25%;
background-position: 25px 0px 0px 0px;
}
.remark-slide-content:after {
content: "Copyright Eli Lilly and Company";
position: absolute;
bottom: -5px;
left: 20px;
height: 40px;
width: 100%;
font-family: Helvetica, Arial, sans-serif;
font-size: 0.7em;
color: #666666;
background-repeat: no-repeat;
background-size: contain;
}
</style>

```{r, include = FALSE, echo = FALSE}
set.seed(0)
suppressPackageStartupMessages({
  library(gittargets)
  library(targets)
})
knitr::opts_chunk$set(
  cache = FALSE,
  comment = "#>",
  fig.width = 10, 
  fig.height = 5
)
```

# Demanding computation in R

* **Bayesian data analysis: JAGS, Stan, NIMBLE, `greta`**
* Deep learning: `keras`, `tensorflow`, `torch`
* Machine learning: `tidymodels`
* PK/PD: `nlmixr`, `mrgsolve`
* Clinical trial simulation: `rpact`, `Mediana`
* Statistical genomics
* Social network analysis
* Permutation tests
* Database queries: `DBI`
* Big data ETL

???

A lot of work that happens in R is computationally demanding. Everything from machine learning to Bayesian statistics to ETL.

---

# Typical notebook-based project

![](./images/notebook-start.png)

???

Most people organize a project using numbered scripts or notebooks. These notebooks run in a sequential order. Each notebook writes output files, and these output files become the input files of the next notebook.

---

# The complicated reality

![](./images/notebook-end.png)

???

This approach does not scale, and the end product is not reproducible. You have to break the linear sequence in order to add steps. Intermediate files are cumbersome to micromanage, the code files get messy, the whole project gets messy, and it's too easy to lose track of what you're doing.

---

# Pipeline tools

![](./images/pipeline_graph.png)

* Orchestrate moving parts.
* Scale the computation.
* Manage output data.
* <span style="color: #ee6600; font-weight: bold;">Manage historical output data</span>.

???

Pipeline tools solve these problems. A pipeline tool orchestrates the overall flow of tasks from beginning to end, running the correct steps in the correct order and conditionally independent tasks in parallel. And most pipeline tools manage output. So for each task, you can trust that all the requisite data files are available and up to date.

---

# {targets}

<center>
<img src="./images/targets.png" height = "300px">
</center>

* Designed for R.
* Encourages good programming habits.
* Behind-the-scenes data management.
* Distributed computing.

???



---

# Example pipeline

```{r, eval = FALSE}
# _targets.R file (get started with use_targets())
library(targets)
library(tarchetypes)
tar_option_set(packages = c("keras", "tidymodels"))
options(clustermq.scheduler = "multisession") # clusters too
source("R/custom_functions.R")

list(
  tar_target(raw_data, "raw_data_file.csv", format = "file"),
  tar_target(tidy_data, munge_data(raw_data), format = "parquet"),
  tar_target(bayesian_model, run_bayesian_model(tidy_data)),
  tar_target(random_forest, run_random_forest(tidy_data)),
  tar_target(deep_neural_net, run_dnn(tidy_data)),
  tar_target(plot_bayes, vis_bayes(bayesian_model)),
  tar_target(plot_rf, vis_rf(random_forest)),
  tar_target(plot_dnn, vis_dnn(deep_neural_net)),
  tar_render(markdown_report, "report.Rmd")
)
```

---

# tar_visnetwork()

![](./images/pipeline_graph.png)

---

# Run the pipeline

```{r, eval = FALSE}
tar_make_clustermq(workers = 2)
#> ✓ skip target raw_data
#> ✓ skip target tidy_data
#> ✓ skip target deep_neural_net
#> • start target bayesian_model
#> • start target random_forest
#> ✓ skip target plot_dnn
#> • built target bayesian_model
#> • start target plot_bayes
#> • built target random_forest
#> • start target plot_rf
#> • built target plot_bayes
#> • built target plot_rf
#> • end pipeline: 30.02 minutes
```

---

# Evidence of reproducibility

```{r, eval = FALSE}
tar_outdated()
#> character(0)
```

```{r, eval = FALSE}
tar_make_clustermq(workers = 2)
#> ✓ skip target raw_data
#> ✓ skip target tidy_data
#> ✓ skip target deep_neural_net
#> ✓ skip target bayesian_model
#> ✓ skip target random_forest
#> ✓ skip target plot_rf
#> ✓ skip target plot_dnn
#> ✓ skip target plot_bayes
#> ✓ skip pipeline: 0.084 seconds
```


---

# Other pipeline tools

* Many exist: https://github.com/pditommaso/awesome-pipeline
* Different tools have different features and tradeoffs.

![](./images/tooltypes.png)

* {[gittargets](https://docs.ropensci.org/gittargets/)} and [cloud integration](https://books.ropensci.org/targets/data.html#cloud-storage) manage historical data for {[targets](https://docs.ropensci.org/targets/)}.

???

Beyond that, different pipeline tools have different priorities. Apache Airflow and GNU Make are two examples. Airflow cares about data pipelines that run on a regular schedule, need to run completely from end to end every time, and need a complete historical record of all past results. Make, on the other hand, only cares about the *current* state of the input and output data, and it cares that the output is synchronized with the code and input it came from. If an output artifact is up to date with its dependencies, then Make will not bother to run that task. This really comes in handy for a data analysis project under active development. If your pipeline tool automatically figures out what to rerun and what to skip, you save an *enormous* amount of time, and you have the peace of mind that your input and output and code are synchronized, up to date, and reproducible.


The {targets} R package is a Make-like tool. It has the benefits of Make, such as skipping tasks that are already up to date. And it also preserves the look and feel of an R session. It nudges users toward good programming practices for the R language, and the interface abstracts away output data files so they look and feel like ordinary variables in R.

---

# Data access in {targets}

```{r, eval = FALSE}
tar_read(tidy_data)
#> # A tibble: 7,043 × 21
#>    customerID gender SeniorCitizen Partner Dependents tenure PhoneService
#>    <chr>      <chr>          <dbl> <chr>   <chr>       <dbl> <chr>       
#>  1 7590-VHVEG Female             0 Yes     No              1 No          
#>  2 5575-GNVDE Male               0 No      No             34 Yes         
#>  3 3668-QPYBK Male               0 No      No              2 Yes         
#>  4 7795-CFOCW Male               0 No      No             45 No          
#>  5 9237-HQITU Female             0 No      No              2 Yes         
#>  6 9305-CDSKC Female             0 No      No              8 Yes         
#>  7 1452-KIOVK Male               0 No      Yes            22 Yes         
#>  8 6713-OKOMC Female             0 No      No             10 No          
#>  9 7892-POOKP Female             0 Yes     No             28 Yes         
#> 10 6388-TABGU Male               0 No      Yes            62 Yes         
#> # … with 7,033 more rows, and 14 more variables: MultipleLines <chr>,
#> #   InternetService <chr>, OnlineSecurity <chr>, OnlineBackup <chr>,
#> #   DeviceProtection <chr>, TechSupport <chr>, StreamingTV <chr>,
#> #   StreamingMovies <chr>, Contract <chr>, PaperlessBilling <chr>,
#> #   PaymentMethod <chr>, MonthlyCharges <dbl>, TotalCharges <dbl>, Churn <chr>
```

---

# {targets} data store

```{r, eval = FALSE}
fs::dir_tree("_targets")
#> _targets
#> ├── meta
#> │   ├── meta
#> │   ├── process
#> │   └── progress
#> ├── objects
#> │   ├── bayesian_model
#> │   ├── deep_neural_net
#> │   ├── plot_bayes
#> │   ├── plot_dnn
#> │   ├── plot_rf
#> │   ├── random_forest
#> │   ├── raw_data
#> │   └── tidy_data
#> └── user
```

???

The data storage system is simple. There is one file per target in the `objects` sub-folder, and the `tar_read()` function brings the data back into the R session. Multiple storage formats and modes of compression are available, including the Apache arrow formats like feather and parquet.

---

# {targets} metadata

```{r, eval = FALSE}
tar_meta() # reads _targets/meta/meta
#> # A tibble: 8 × 18
#>   name     type  data  command depend    seed path  time                size  bytes format
#>   <chr>    <chr> <chr> <chr>   <chr>    <int> <lis> <dttm>              <chr> <int> <chr> 
#> 1 bayesia… stem  df47… 2d2dc8… f3915…  7.53e8 <chr> 2022-05-11 14:38:15 9294…  1276 rds   
#> 2 deep_ne… stem  df47… 2d2dc8… f3915… -2.15e8 <chr> 2022-05-11 14:37:17 9294…  1276 rds   
#> 3 plot_ba… stem  df47… ac4f51… 0a2b8…  1.70e9 <chr> 2022-05-11 14:38:15 9294…  1276 rds   
#> 4 plot_dnn stem  df47… 8782dd… 93270… -1.97e9 <chr> 2022-05-11 14:37:17 9294…  1276 rds   
#> 5 plot_rf  stem  df47… 90b285… 104bf…  1.37e9 <chr> 2022-05-11 14:38:15 9294…  1276 rds   
#> 6 random_… stem  df47… 2d2dc8… f3915…  4.76e8 <chr> 2022-05-11 14:38:15 9294…  1276 rds   
#> 7 raw_data stem  df47… ff67f4… 933dc… -9.80e8 <chr> 2022-05-11 14:37:17 9294…  1276 rds   
#> 8 tidy_da… stem  df47… 698a55… 4c4de… -1.09e9 <chr> 2022-05-11 14:37:17 9294…  1276 rds   
#> # … with 7 more variables: repository <chr>, iteration <chr>, parent <lgl>,
#> #   children <list>, seconds <dbl>, warnings <lgl>, error <lgl>
```

---

# Core {targets} data management

<br>

![](./images/tradeoffs.png)

<br>

## Solutions

1. Cloud integration: [books.ropensci.org/targets/data.html#cloud-storage](https://books.ropensci.org/targets/data.html#cloud-storage)
2. {gittargets}: [docs.ropensci.org/gittargets/](https://docs.ropensci.org/gittargets/)

---

# Solution 1: cloud storage

1. Create a bucket (either Amazon or Google) and turn on versioning.
2. Supply the bucket to `tar_resources()`.
3. Select `repository = "aws"` (Amazon) or `"gcp"` (Google).
4. Commit the metadata file `_targets/meta/meta` to the same version control repository as the analysis code.

## Benefits

* Rolling back the code also rolls back the data.
* If your targets were up to date when the commit was created, they stay up to date after the rollback.
* More: <https://books.ropensci.org/targets/data.html#cloud-storage>.

---

# Target script with cloud storage

```{r, eval = FALSE}
# _targets.R file (get started with use_targets())
library(targets)
tar_option_set(
  packages = c("keras", "tidymodels"),
  resources = tar_resources(
    aws = tar_resources_aws(bucket = "my_versioned_bucket") #<<
  )
)
# ...

list(
  # ...
  tar_target(
    bayesian_model,
    run_bayesian_model(tidy_data),
    repository = "aws" # or "gcp" #<<
  )
  # ...
)
```


???

First, the {targets} package itself supports storage in the cloud. If you supply a bucket in Amazon Web Services or Google Cloud, you can send target data there instead of your local machine. And if you turn on versioning in your bucket, {targets} automatically stores the version ID of each results file. That means if you track the project's metadata file in the same version control repository as the source code, all your past runs will be synchronized and available. In other words, when you check out an old Git commit, your data will roll back too, and your targets will stay up to date. And because up-to-date targets are skipped, this approach avoids duplicating output data like an Airflow tool might if it runs the full pipeline from scratch each time.

---

# Solution 2: {gittargets}

<center>
<img src="./images/gittargets.png" height = "300px">
</center>

* Puts the data under version control outside the code repository.
* Syncs data snapshots with contemporaneous code commits.
* Rolls back data when you roll back code so your targets stay up to date.
* Documentation: https://docs.ropensci.org/gittargets/

???

For those of you without access to Amazon Web Services or Google Cloud, the {gittargets} R package is a completely local solution. {gittargets} creates a Git repository for the data store that stays synchronized with the Git repository of your source code. That way, when you check out an old commit in the code, it is straightforward to pull up the data snapshot that matches the commit. 

---

# {gittargets} workflow

| Package | Function | Action
---|---|---|---
1 | `targets` | `tar_make()` | Run the pipeline.
2 | `gert` | `git_commit()` | Commit code to version control.
3 | `gittargets` | `tar_git_init()` | Create data repository.
4 | `gittargets` | `tar_git_snapshot()` | Snapshot the data.
5 | `gittarget` | `tar_git_checkout()` | Roll back data to match code.

???

Here's how it works. First, run the pipeline and commit your code to version control. Your targets should be up to date in the pipeline, and your code should be up to date in Git. Next, use tar_git_init() to create a Git repository for the {targets} data store, and use tar_git_snapshot() to commit the data files. Now, if you ever need to roll back the code to this commit, you can use tar_git_checkout() to roll back the data. If all goes well, you will have an older but still up-to-date set of targets.

Let's demonstrate. 

---

# Initialize data repository

* Assumes the pipeline ran at least once.

```{r, eval = FALSE}
tar_git_init()
#> ✔ Created data store Git repository
#> ✔ Wrote to _targets/.gitattributes for git-lfs: <https://git-lfs.github.com>.
#> ✔ Created stub commit without data.
#> • Run tar_git_snapshot() to put the data files under version control.
```


???

Initializing the data repository is straightforward,

---

# Snapshot the data


```{r, eval = FALSE}
tar_git_snapshot()
#> • Creating data branch code=af36d7301844831adc4cdedffbcb802cecebb0d1.
#> • Staging data files.
#> ✔ Staged 11 files in the data store.
#> • Committing data changes.
#> ✔ Created new data snapshot b0641526e062fee82e5d3d2c88163ea4823bc1b8.
#> • Packing references.
```

???

and so is snapshotting the data.

---

# Snapshot model

<center>
<img src="./images/snapshot-model-git.png" height = "450px">
</center>

???

A snapshot is a special git commit in the data that links to a commit in the code. All the data snapshots that match a code commit are in a branch with the code commit hash.

---

# Check out old code

```{r, eval = FALSE}
gert::git_branch_checkout("old-code-branch")
```

* Find code commits with data snapshots:

```{r, eval = FALSE}
tar_git_log()
#> # A tibble: 2 × 6
#>   message_code  message_data time_code           time_data           commit_code
#>   <chr>         <chr>        <dttm>              <dttm>              <chr>      
#> 1 First commit  First commi… 2022-02-13 01:32:53 2022-02-13 01:32:55 e8ad30b018…
#> 2 Update data   Update data  2022-02-13 01:32:56 2022-02-13 01:32:57 777b678f85…
#> # … with 1 more variable: commit_data <chr>
```

???

When you check out an old branch, you may or may not have a data snapshot for the code commit you switched to. If you are unsure, you can call tar_git_log() to find out.

---

# Check out matching old data

```{r, eval = FALSE}
tar_git_checkout()
#> ✔ Checked out data snapshot b0641526e062fee82e5d3d2c88163ea4823bc1b8.
#> • Code commit: code=e8ad30b01844831adc4cdedffbcb802cecebb0d1
#> • Message: First commit
#> • Resetting to HEAD of checked-out snapshot.
```

```{r, eval = FALSE}
tar_make() # The old data files are synced with the old code.
#> ✓ skip target raw_data
#> ✓ skip target tidy_data
#> ✓ skip target deep_neural_net
#> ✓ skip target bayesian_model
#> ✓ skip target random_forest
#> ✓ skip target plot_rf
#> ✓ skip target plot_dnn
#> ✓ skip target plot_bayes
#> ✓ skip pipeline: 0.092 seconds
```

???

If you do have a matching snapshot, you can call tar_git_checkout() to pull up the data that matches the code commit. And now, you have an prior set of targets that matches this historical state of the code. 

---

# Thanks

* [Mark Edmondson](https://github.com/MarkEdmondson1234) contributed Google Cloud Storage support in {targets}.
* [rOpenSci](https://ropensci.org/) reviewed, adopted, and promoted {targets} and {gittargets}.
* rOpenSci [reviewers](https://github.com/ropensci/software-review/issues/401) of {targets} and {tarchetypes}:
    * [Samantha Oliver](https://github.com/limnoliver)
    * [TJ Mahr](https://github.com/tjmahr)
* rOpensci [reviewers](https://github.com/ropensci/software-review/issues/486) of {gittargets}:
    * [Saras Windecker](https://github.com/smwindecker)
    * [David Neuzerling](https://github.com/mdneuzerling)
